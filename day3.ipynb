{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83f11022",
   "metadata": {},
   "source": [
    "# Contextualized token embeddings for semantic change detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0b1617",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plot\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr\n",
    "from scipy.spatial.distance import cosine, cdist\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn import preprocessing\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8801fe1f",
   "metadata": {},
   "source": [
    "# Loading the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287a15b3",
   "metadata": {},
   "source": [
    "For simplicity, we are using a version of SemEval'20 English test set *without POS tags*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17248820",
   "metadata": {},
   "outputs": [],
   "source": [
    "graded = pd.read_csv(\"targets/english/graded_nopos.txt\", sep=\"\\t\", header=None,\n",
    "                     names=['word', 'truth'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fd3c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "graded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d300083",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = graded.word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ceafed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"Target lemmas: {len(targets)}.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbd5604",
   "metadata": {},
   "source": [
    "Again, corpus1 is XIX century English, corpus 2 is XX century English"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ca79c6",
   "metadata": {},
   "source": [
    "# 1. Embedding part"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddccd8b9",
   "metadata": {},
   "source": [
    "We assume the token embeddings are already extracted using the language model of our choice (BERT, XLM-R, etc).\n",
    "If you are curious, look at the `extract.py` script.\n",
    "Embeddings are stored as Numpy matrices (compressed). They are about 200 MBytes each, so we publish them separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcea1a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -c https://www.mn.uio.no/ifi/english/people/aca/andreku/token_embeddings_corpus1_xlmr.npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582c12bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -c https://www.mn.uio.no/ifi/english/people/aca/andreku/token_embeddings_corpus2_xlmr.npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9199b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -c https://www.mn.uio.no/ifi/english/people/aca/andreku/token_embeddings_corpus1_bert.npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e046e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -c https://www.mn.uio.no/ifi/english/people/aca/andreku/token_embeddings_corpus2_bert.npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59324a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path1= \"token_embeddings_corpus1_bert.npz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268221c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "array1 = np.load(data_path1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586ddf8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"Loaded an array of {len(array1)} entries from {data_path1}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e95bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path2 = \"token_embeddings_corpus2_bert.npz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820e660b",
   "metadata": {},
   "outputs": [],
   "source": [
    "array2 = np.load(data_path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3cb51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"Loaded an array of {len(array2)} entries from {data_path2}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009fb674",
   "metadata": {},
   "source": [
    "# Visualizing token embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a239924",
   "metadata": {},
   "outputs": [],
   "source": [
    "word = \"plane\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e962dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "array = array2[word]\n",
    "array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5a8f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = PCA(n_components=2)\n",
    "y = embedding.fit_transform(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84359774",
   "metadata": {},
   "outputs": [],
   "source": [
    "xpositions = y[:, 0]\n",
    "ypositions = y[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c09009e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.clf()\n",
    "plot.scatter(xpositions, ypositions, 5, marker='*', color='green')\n",
    "plot.tick_params(axis='x', which='both', bottom=False, top=False, labelbottom=False)\n",
    "plot.tick_params(axis='y', which='both', left=False, right=False, labelleft=False)\n",
    "plot.title(f\"{word} in {data_path2}\")\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57855449",
   "metadata": {},
   "source": [
    "Every dot is a 768-dimensional token embeddings projected into 2 dimensions.\n",
    "Can you show both time periods side by side?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a389635a",
   "metadata": {},
   "source": [
    "We can show two time periods on one plot as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae182519",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = {\"bin1\": array1[word], \"bin2\": array2[word]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509c601c",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings[\"bin2\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e972dc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.concatenate([embeddings[el] for el in sorted(embeddings)], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748c81f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92389b1",
   "metadata": {},
   "source": [
    "We want to show usages from different time bins with different colors, thus, we need class labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875b37fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels = []\n",
    "for el in sorted(embeddings):\n",
    "    class_labels += [el] * len(embeddings[el])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b200e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(class_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bd13fe",
   "metadata": {},
   "source": [
    "We are projecting all embeddings into 2 dimensions with PCA or TSNE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c8ef8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = preprocessing.StandardScaler().fit_transform(x)\n",
    "x_2d = PCA(n_components=2).fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c755dcc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_set = sorted([c for c in set(class_labels)])\n",
    "colors = plot.cm.Dark2(np.linspace(1, 0, len(class_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2443577",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.clf\n",
    "plot.figure(figsize=(15, 15))\n",
    "plot.xticks([]), plot.yticks([])\n",
    "plot.title(f\"{word} in all time bins\\n\", fontsize=20)\n",
    "for year in class_set:\n",
    "    rows = [x == year for x in class_labels]\n",
    "    matrix = x_2d[rows]\n",
    "    plot.scatter(matrix[:, 0], matrix[:, 1], color=colors[class_set.index(year)], marker='*', s=40, label=year)\n",
    "plot.legend(prop={'size': 15}, loc=\"best\")\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6163682",
   "metadata": {},
   "source": [
    "What we will need to be able to inspect the actual usages? How to annotate the dots with the identifiers pointing at real sentences?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf2ff43",
   "metadata": {},
   "source": [
    "Anyway, now we would like to use the token embeddings to quantitatively estimate the degree of semantic change. And here comes the..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea586ec",
   "metadata": {},
   "source": [
    "# 2. Aggregating and assessment part"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52b296a",
   "metadata": {},
   "source": [
    "There are many usages and many token embeddings. We need to somehow *aggregate* them for each time period, before *assessing* the change."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abd2890",
   "metadata": {},
   "source": [
    "The simplest is the PRT method (comparison of averaged *prototypical* embeddings):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab363547",
   "metadata": {},
   "source": [
    "## PRT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87423f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prt_predictions = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0acd1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in sorted(targets):\n",
    "    frequency = np.sum([array1[word].shape[0], array2[word].shape[0]])\n",
    "    vectors1 = array1[word]\n",
    "    vectors2 = array2[word]\n",
    "    vectors = []\n",
    "    for m in [vectors1, vectors2]:\n",
    "        # Aggregation:\n",
    "        vector = np.average(m, axis=0)\n",
    "        vectors.append(vector)\n",
    "    vectors = [preprocessing.normalize(v.reshape(1, -1), norm='l2') for v in vectors]\n",
    "    # Assessment:\n",
    "    shift = 1 - np.dot(vectors[0].reshape(-1), vectors[1].reshape(-1))\n",
    "    prt_predictions.append(shift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1badced2",
   "metadata": {},
   "outputs": [],
   "source": [
    "graded[\"prt_predictions\"] = prt_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b08b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "graded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97664a89",
   "metadata": {},
   "source": [
    "Let's evaluate the predictions. How correlated they are with the ground truth (human judgments)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e232a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation = spearmanr(graded.truth, graded.prt_predictions)\n",
    "print(f\"Spearman ranked correlation for PRT: {correlation[0]:0.4f}; p-value: {correlation[1]:0.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a345a0",
   "metadata": {},
   "source": [
    "## APD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7adf186e",
   "metadata": {},
   "source": [
    "Average Pairwise Distance (APD) is a more sophisticated aggreation method. It computes pairwise distances between *all* usages from two time bins and averages these distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c817cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_pairwise_distance(usage_matrix1, usage_matrix2, metric=\"cosine\"):\n",
    "    \"\"\"\n",
    "    Computes the mean pairwise distance between two usage matrices.\n",
    "\n",
    "    :param word_usages1: usage matrix 1\n",
    "    :param word_usages2: usage matrix 2\n",
    "    :param metric: a distance metric compatible with `scipy.spatial.distance.cdist`\n",
    "    (e.g. 'cosine', 'euclidean')\n",
    "    :return: the mean pairwise distance between two usage matrices\n",
    "    \"\"\"\n",
    "    if usage_matrix1.shape[0] == 0 or usage_matrix2.shape[0] == 0:\n",
    "        raise ValueError('Zero-dimensional usage matrix.')\n",
    "    # cdist is the most computationally expensive operation here\n",
    "    return np.mean(cdist(usage_matrix1, usage_matrix2, metric=metric))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ca008b",
   "metadata": {},
   "source": [
    "Computational complexity naturally grows quadratically with the number of usages, so we will introduce sampling of max 5 000 random usages from each time bin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90a198c",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_samples = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5773ac83",
   "metadata": {},
   "outputs": [],
   "source": [
    "apd_predictions = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b10702",
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in sorted(targets):\n",
    "    frequency = np.sum([array1[word].shape[0], array2[word].shape[0]])\n",
    "    print(f\"Processing {word} with the total frequency {frequency}...\")\n",
    "    vectors1 = array1[word]\n",
    "    vectors2 = array2[word]\n",
    "    if vectors1.shape[0] > max_samples:\n",
    "        prev = vectors1.shape[0]\n",
    "        rand_indices = np.random.choice(prev, max_samples, replace=False)\n",
    "        vectors1 = vectors1[rand_indices]\n",
    "        print(f\"Choosing {max_samples} random usages from {prev} for {word} in T0\")\n",
    "    if vectors2.shape[0] > max_samples:\n",
    "        prev = vectors2.shape[0]\n",
    "        rand_indices = np.random.choice(prev, max_samples, replace=False)\n",
    "        vectors2 = vectors2[rand_indices]\n",
    "        print(f\"Choosing {max_samples} random usages from {prev} for {word} in T1\")\n",
    "    shift = mean_pairwise_distance(vectors1, vectors2, \"cosine\")\n",
    "    apd_predictions.append(shift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9128d4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "graded[\"apd_predictions\"] = apd_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5ff351",
   "metadata": {},
   "outputs": [],
   "source": [
    "graded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d241122d",
   "metadata": {},
   "source": [
    "Again, let's evaluate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5354c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation = spearmanr(graded.truth, graded.apd_predictions)\n",
    "print(f\"Spearman ranked correlation for APD: {correlation[0]:0.4f}; p-value: {correlation[1]:0.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a98fb8",
   "metadata": {},
   "source": [
    "## PRT/APD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b66ace",
   "metadata": {},
   "source": [
    "One can also take the geometric mean of PRT and APD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7b778a",
   "metadata": {},
   "outputs": [],
   "source": [
    "graded[\"prt_apd_predictions\"] = graded.apply(lambda row: np.sqrt(row.prt_predictions * row.apd_predictions), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c294c27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "graded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b51c89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation = spearmanr(graded.truth, graded.prt_apd_predictions)\n",
    "print(f\"Spearman ranked correlation for PRT/APD: {correlation[0]:0.4f}; p-value: {correlation[1]:0.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cbbb8f",
   "metadata": {},
   "source": [
    "You can try other languages!\n",
    "Annotated datasets are available here: https://www.ims.uni-stuttgart.de/en/research/resources/experiment-data/wugs/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
