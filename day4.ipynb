{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b840d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.stats import spearmanr \n",
    "from scipy.spatial.distance import cosine\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bafcc8",
   "metadata": {},
   "source": [
    "# Loading data\n",
    "\n",
    "In this session we will work with the __[Latin SemEval dataset](https://zenodo.org/record/3734089)__. We preprocessed corpus with UDPipe and extracted *profiles*, i.e. counts for each target-word form in each corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4f30e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "properties_1 = json.load(open(\"features/latin_corpus1_morph.json\", \"r\"))\n",
    "properties_2 = json.load(open(\"features/latin_corpus2_morph.json\", \"r\"))                  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccce5e12",
   "metadata": {},
   "source": [
    "For example, in the first corpus word `imperator` was used 80 times in Nominative case with Masculine gender and Plural Number, while in the second corpus it was used in that specific form 3522 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71809ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "properties_1['imperator']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ad55d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "properties_2['imperator']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc5dcb4",
   "metadata": {},
   "source": [
    "# Cosine similarity\n",
    "\n",
    "We can use each category combination as a feature and measure semantic shift as cosine similarity between vectors in this feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04669247",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_properties(word):\n",
    "    return properties_1[word], properties_2[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32e4da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(prop1, prop2, thr=0):\n",
    "    \n",
    "    # features is a combination of dictionary keys from two periods\n",
    "    features = {k:prop1.get(k,0)+prop2.get(k,0) for k in set(prop1.keys()).union(set(prop2.keys()))}\n",
    "    \n",
    "    # FILTERING:\n",
    "    # default: no filtering\n",
    "    features = {k:v for k,v in features.items() if v>thr}\n",
    "    \n",
    "    # lets set a count to 0 if a feature is missed in a dictionary\n",
    "    counts1 = [prop1.get(f,0) for f in features]\n",
    "    counts2 = [prop2.get(f,0) for f in features]\n",
    "    \n",
    "    # now we can compute a score\n",
    "    return cosine(counts1, counts2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443c82bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "score(*get_properties('imperator'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603417ee",
   "metadata": {},
   "source": [
    "Now lets load the ground truth, compute score for all target words and calculate the method performance using Spearman rank correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59be3c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load target words with ground truth\n",
    "graded = pd.read_csv('targets/latin/graded.txt', sep=\"\\t\", header=None, names=['word', 'truth'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a889544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute score for each word in the list\n",
    "graded[\"score\"] = graded.apply(lambda row: score(*get_properties(row.word)), axis = 1)\n",
    "graded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60747b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate using Spearman Rank Correlation\n",
    "spearmanr(graded.truth, graded.score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfbbc98",
   "metadata": {},
   "source": [
    "**Your turn** Try to add filtering, i.e. removing features that appear in both corpora less than 5 times in total (use `thr` parameter). What happens with the correlation score? Why? Try different values for the threshold. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62f3783",
   "metadata": {},
   "source": [
    "# Split features\n",
    "\n",
    "In the section above the feature space was constructed from word forms: case, number and other grammatical properties were used in combination. However, we can split them and count each property separately. For example, we can count how many times a word has been used in a plural form, regardless the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4fa02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function that splits properties\n",
    "def split_props(properties):\n",
    "    splt = defaultdict(int)\n",
    "    for p,count in properties.items():\n",
    "        for f in p.split(\"|\"):\n",
    "            splt[f] += count\n",
    "    return splt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af7ed3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_props(properties_1['imperator'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059fa670",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_split_props(word):\n",
    "    return split_props(properties_1[word]), split_props(properties_2[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0652259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "graded[\"split_score\"] = graded.apply(lambda row: score(*get_split_props(row.word)), axis = 1)\n",
    "graded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc338a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "spearmanr(graded.truth, graded.split_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9acdb15",
   "metadata": {},
   "source": [
    "**Your turn** Check effect of various filtering thresholds in this method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f447cd79",
   "metadata": {},
   "source": [
    "# Two-step\n",
    "\n",
    "In the previous section, we used various morphological properties all together. `Case=Nom`, `Case=Acc` and `Number=Sing` were all treated equally, even though the first two are mutually exclusive while both can be combined with the third one. It can have more sense to compute distances for each morphological category separately, e.g. number distance, case distance and so forth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6549337c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function that splits properties by morphological category\n",
    "def two_step_split(properties):\n",
    "    splt = defaultdict(lambda: defaultdict(int))\n",
    "    for p,count in properties.items():\n",
    "        for f in p.split(\"|\"):\n",
    "            try:\n",
    "                category, value = f.split(\"=\")\n",
    "            except ValueError:  #not enough values to unpack \n",
    "                continue\n",
    "            splt[category][value] += count \n",
    "    return splt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f8e031",
   "metadata": {},
   "outputs": [],
   "source": [
    "two_step_split(properties_1['imperator'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fd0417",
   "metadata": {},
   "source": [
    "Now for each word we get a *set* of scores, one for each morphological category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ae303e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_step_score(word, thr=0):\n",
    "    prop1 = two_step_split(properties_1[word])\n",
    "    prop2 = two_step_split(properties_2[word])\n",
    "    \n",
    "    categories = {k:sum(prop1[k].values())+sum(prop2[k].values()) \n",
    "                  for k in set(prop1.keys()).union(set(prop2.keys()))}\n",
    "    \n",
    "    #filtering:\n",
    "    total = sum(categories.values())\n",
    "    categories = {k:v for k,v in categories.items() if v > thr }\n",
    "    \n",
    "    scores = {cat:score(prop1[cat], prop2[cat]) for cat in categories}\n",
    "    \n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1237fc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "two_step_score('imperator', thr=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94db4d8f",
   "metadata": {},
   "source": [
    "We can compute the overall change score by *averaging* scores for each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240b199b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregated_score(scores):\n",
    "    return np.mean(list(scores.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb75f14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "graded[\"two_step_score\"] = graded.apply(lambda row: aggregated_score(two_step_score(row.word)), axis = 1)\n",
    "graded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d1ab07",
   "metadata": {},
   "outputs": [],
   "source": [
    "spearmanr(graded.truth, graded.two_step_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79f272d",
   "metadata": {},
   "source": [
    "**Your turn:** \n",
    "<br>\n",
    "1. Try to use maximum of the scores instead of averaging. How this affects the results? Why?\n",
    "<br>\n",
    "2. The threshold we are using here is the same for all words (e.g. 5), while in the paper we used a variable threshold, 5% of total word count. Implement this and see how it will affect the results. Hint: use `total` defined inside the `two_step_score` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ad0dcc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
